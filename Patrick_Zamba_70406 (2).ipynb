{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "XMgYvTKKOBvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "eB7lLsAjN-5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n02LYpmFCyT"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir ~/.kaggle\n",
        "#!cp kaggle.json ~/.kaggle/  # Upload your Kaggle API token as 'kaggle.json'\n",
        "#!chmod 600 ~/.kaggle/kaggle.json\n",
        "#!kaggle datasets download -d uciml/sms-spam-collection-dataset\n",
        "#!unzip sms-spam-collection-dataset.zip\n",
        "\n",
        "# 1.Load the dataset\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df[['v1', 'v2']]  # Select relevant columns\n",
        "df.columns = ['label', 'text']"
      ],
      "metadata": {
        "id": "ahuLJ8I8NEB8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove non-text characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Clean text data\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n",
        "# Convert labels to binary\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "mKIwUceFPE_J"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "cknvHIQvlGlo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Encoding\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "2GLrRsUfmdY_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "0GPr7lN3mmts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "N6WqkZVKmqUR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(\"Random Forest Model Accuracy:\", accuracy_rf)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_rf)"
      ],
      "metadata": {
        "id": "FyRSMJkwm0Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.TF-IDF Vectorization######\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "X = tfidf.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Encode labels\n",
        "y = df['label'].apply(lambda x: 1 if x == 'spam' else 0)"
      ],
      "metadata": {
        "id": "x25b7ZiaPqNy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "J1QgpkKCP2PO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Train the model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MVnDLpAjP8ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "9R6uc9o-UJSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 5.Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "n7ZEUhkYQZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(\"Random Forest Model Accuracy:\", accuracy_rf)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_rf)\n"
      ],
      "metadata": {
        "id": "MrhCEAi8nx55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stage 2\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "PvrAQ20OQgdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.tokenise & pad sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "X = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 100\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "y = df['label'].apply(lambda x: 1 if x == 'spam' else 0).values"
      ],
      "metadata": {
        "id": "nznVnGrAQzXh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.convolutional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Define the model\n",
        "embedding_dim = 50\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=embedding_dim, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "xBlZ4xAQSFHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.evaluating model\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "3wO18c_FZGwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3word2vector\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [text.split() for text in df['text']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create a weight matrix for words in the training set\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "vGwSUzjbZZl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stage 3BERT embedding\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "KUlxCwJqaJVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def bert_encode(texts, tokenizer, max_len=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids), np.array(attention_masks)\n",
        "\n",
        "# Prepare data\n",
        "X_input_ids, X_attention_masks = bert_encode(df['text'], tokenizer)\n"
      ],
      "metadata": {
        "id": "jtKx132YbE8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None)\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "X_train_encoded = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "X_test_encoded = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Get BERT embeddings for train and test sets\n",
        "X_train_embedded = model(X_train_encoded.input_ids, attention_mask=X_train_encoded.attention_mask)[0]\n",
        "X_test_embedded = model(X_test_encoded.input_ids, attention_mask=X_test_encoded.attention_mask)[0]\n",
        "\n",
        "# Define and compile the DL model\n",
        "input_layer = Input(shape=(X_train_embedded.shape[1], X_train_embedded.shape[2]))\n",
        "dropout_layer = Dropout(0.2)(input_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_embedded, y_train, epochs=3, batch_size=32, validation_data=(X_test_embedded, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_embedded)\n",
        "y_pred_binary = np.round(y_pred).astype(int)\n",
        "accuracy_bert = accuracy_score(y_test, y_pred_binary)\n",
        "conf_matrix_bert = confusion_matrix(y_test, y_pred_binary)\n",
        "\n",
        "print(\"BERT Model Accuracy:\", accuracy_bert)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_bert)\n"
      ],
      "metadata": {
        "id": "ky73hLFOb4nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
        "    X_input_ids, X_attention_masks, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_ids, X_train_masks], y_train,\n",
        "    epochs=3, batch_size=16, validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate([X_test_ids, X_test_masks], y_test)\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = (model.predict([X_test_ids, X_test_masks]) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "oj8H9oOpcMVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None)\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "X_train_encoded = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "X_test_encoded = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Get BERT embeddings for train and test sets\n",
        "X_train_embedded = model(X_train_encoded.input_ids, attention_mask=X_train_encoded.attention_mask)[0]\n",
        "X_test_embedded = model(X_test_encoded.input_ids, attention_mask=X_test_encoded.attention_mask)[0]\n",
        "\n",
        "# Define and compile the DL model\n",
        "input_layer = Input(shape=(X_train_embedded.shape[1], X_train_embedded.shape[2]))\n",
        "dropout_layer = Dropout(0.2)(input_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_embedded, y_train, epochs=3, batch_size=32, validation_data=(X_test_embedded, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_embedded)\n",
        "y_pred_binary = np.round(y_pred).astype(int)\n",
        "accuracy_bert = accuracy_score(y_test, y_pred_binary)\n",
        "conf_matrix_bert = confusion_matrix(y_test, y_pred_binary)\n",
        "\n",
        "print(\"BERT Model Accuracy:\", accuracy_bert)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_bert)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_bert, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={'fontsize': 14})\n",
        "plt.xlabel('Predicted Labels', fontsize=14)\n",
        "plt.ylabel('True Labels', fontsize=14)\n",
        "plt.title('Confusion Matrix - BERT Model', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WfqZhzvFwl15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}